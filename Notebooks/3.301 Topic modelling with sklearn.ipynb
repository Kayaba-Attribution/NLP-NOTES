{"cells":[{"cell_type":"markdown","id":"5edf24b9","metadata":{"id":"5edf24b9"},"source":["# Topic modelling\n","## Unsupervised learning of topics in a text\n","### using Latent Dirchlet Allocation (via sklearn)\n","Topic modelling can be thought of as dimensionality reduction:  \n","Documents are represented as sets of topics  \n","Each topic has a weight"]},{"cell_type":"code","execution_count":22,"id":"e8307b6e","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"e8307b6e","executionInfo":{"status":"ok","timestamp":1670423260370,"user_tz":0,"elapsed":1760,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}},"outputId":"8504aaf7-c463-4a17-d456-b71e516fb62d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}],"source":["import re\n","import pandas as pd\n","import sklearn\n","import csv\n","import nltk\n","import string\n","from nltk.stem.snowball import SnowballStemmer\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":2,"id":"8e0cace1","metadata":{"id":"8e0cace1","executionInfo":{"status":"ok","timestamp":1670422443787,"user_tz":0,"elapsed":387,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# use CountVectorizer to turn the docs into vectors\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation as LDA"]},{"cell_type":"code","execution_count":3,"id":"e2211f19","metadata":{"id":"e2211f19","executionInfo":{"status":"ok","timestamp":1670422445656,"user_tz":0,"elapsed":10,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# create the stemmer\n","stemmer = SnowballStemmer('english')"]},{"cell_type":"code","execution_count":8,"id":"d61f82c1","metadata":{"tags":[],"id":"d61f82c1","executionInfo":{"status":"ok","timestamp":1670422529449,"user_tz":0,"elapsed":732,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# helper functions\n","stopwords_file_path = \"stopwords.csv\"\n","\n","def read_in_csv(csv_file):\n","    with open(csv_file, 'r', encoding='utf-8') as fp:\n","        reader = csv.reader(fp, delimiter=',', quotechar='\"')\n","        data_read = [row for row in reader]\n","    return data_read\n","\n","def get_stopwords(path=stopwords_file_path):\n","    stopwords = read_in_csv(path)\n","    stopwords = [word[0] for word in stopwords]\n","    stemmed_stopwords = [stemmer.stem(word) for word in stopwords]\n","    stopwords = stopwords + stemmed_stopwords\n","    return stopwords\n","\n","def tokenize_and_stem(sentence):\n","    tokens = nltk.word_tokenize(sentence)\n","    filtered_tokens = [t for t in tokens if t not in stopwords and t not in string.punctuation and re.search('[a-zA-Z]', t)]\n","    stems = [stemmer.stem(t) for t in filtered_tokens]\n","    return stems"]},{"cell_type":"code","source":["# if you're on colab upload the data files\n","from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"L1PHHgSu9z4T","executionInfo":{"status":"ok","timestamp":1670423049125,"user_tz":0,"elapsed":98076,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}},"outputId":"34a5b8ec-b350-4412-8452-066519c02d0a"},"id":"L1PHHgSu9z4T","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-e4df0c53-d7f0-4260-b56e-d9cc5387c847\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e4df0c53-d7f0-4260-b56e-d9cc5387c847\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving bbc-text.csv to bbc-text.csv\n","Saving stopwords.csv to stopwords.csv\n"]}]},{"cell_type":"code","execution_count":19,"id":"c0dce000","metadata":{"tags":[],"id":"c0dce000","executionInfo":{"status":"ok","timestamp":1670423234359,"user_tz":0,"elapsed":595,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# read in our data\n","stopwords = get_stopwords(stopwords_file_path)\n","bbc_dataset = \"bbc-text.csv\""]},{"cell_type":"markdown","id":"7720956a","metadata":{"id":"7720956a"},"source":["### We’ll use a public dataset from the BBC comprised of 2,225 articles  \n","Each labeled under one of 5 categories: business, entertainment, politics, sport or tech"]},{"cell_type":"code","execution_count":10,"id":"9592600c","metadata":{"id":"9592600c","executionInfo":{"status":"ok","timestamp":1670422847110,"user_tz":0,"elapsed":601,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# turn the documents into vectors\n","def create_count_vectorizer(documents):\n","    count_vectorizer = CountVectorizer(stop_words=stopwords, tokenizer=tokenize_and_stem, max_features=1500)\n","    data = count_vectorizer.fit_transform(documents)\n","    return (count_vectorizer, data)"]},{"cell_type":"code","execution_count":11,"id":"0edde963","metadata":{"id":"0edde963","executionInfo":{"status":"ok","timestamp":1670422850238,"user_tz":0,"elapsed":5,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# remove unwanted characters (keep just words and spaces)\n","def clean_data(df):\n","    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n","    df['text'] = df['text'].apply(lambda x: re.sub(r'\\d', '', x))\n","    return df"]},{"cell_type":"code","execution_count":12,"id":"0d32dd2f","metadata":{"id":"0d32dd2f","executionInfo":{"status":"ok","timestamp":1670422852694,"user_tz":0,"elapsed":362,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# create the LDA model (note that usually num_topics is unknown)\n","def create_and_fit_lda(data, num_topics):\n","    lda = LDA(n_components=num_topics, n_jobs=-1)\n","    lda.fit(data)\n","    return lda"]},{"cell_type":"code","execution_count":13,"id":"f2b5f2ed","metadata":{"id":"f2b5f2ed","executionInfo":{"status":"ok","timestamp":1670422855918,"user_tz":0,"elapsed":880,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# identify & print the most common topic words\n","def get_most_common_words_for_topics(model, vectorizer, n_top_words):\n","    words = vectorizer.get_feature_names()\n","    word_dict = {}\n","    for topic_index, topic in enumerate(model.components_):\n","        this_topic_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n","        word_dict[topic_index] = this_topic_words\n","    return word_dict\n","\n","def print_topic_words(word_dict):\n","    for key in word_dict.keys():\n","        print(f\"Topic {key}\")\n","        print(\"\\t\", word_dict[key])"]},{"cell_type":"code","execution_count":20,"id":"7e904d14","metadata":{"id":"7e904d14","executionInfo":{"status":"ok","timestamp":1670423241541,"user_tz":0,"elapsed":365,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# read in the data, clean it, get text\n","df = pd.read_csv(bbc_dataset)\n","df = clean_data(df)\n","documents = df['text']\n","\n","# set number of topics (note that usually this is unknown)\n","number_topics = 5"]},{"cell_type":"code","execution_count":23,"id":"a1a3a80f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1a3a80f","executionInfo":{"status":"ok","timestamp":1670423298997,"user_tz":0,"elapsed":35152,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}},"outputId":"2ed6baa8-9de6-46fa-93a5-08979af994e9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['l', 'v'] not in stop_words.\n","  warnings.warn(\n"]}],"source":["# create vectorizer & model\n","(vectorizer, data) = create_count_vectorizer(documents)\n","lda = create_and_fit_lda(data, number_topics)"]},{"cell_type":"code","execution_count":24,"id":"b5e7e5ce","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5e7e5ce","executionInfo":{"status":"ok","timestamp":1670423308510,"user_tz":0,"elapsed":392,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}},"outputId":"abbdd31e-f92e-4759-f021-958af1c26448"},"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 0\n","\t ['use', 'peopl', 'game', 'mobil', 'technolog', 'phone', 'servic', 'music', 'new', 'user']\n","Topic 1\n","\t ['govern', 'countri', 'peopl', 'new', 'year', 'say', 'work', 'report', 'world', 'uk']\n","Topic 2\n","\t ['year', 'm', 'film', 'play', 'best', 'game', 'win', 'first', 'time', 'award']\n","Topic 3\n","\t ['labour', 'elect', 'parti', 'say', 'blair', 'govern', 'minist', 'tori', 'peopl', 'brown']\n","Topic 4\n","\t ['year', 'bn', 'compani', 'm', 'market', 'firm', 'sale', 'price', 'share', 'bank']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["# inspect the contents of the topics\n","topic_words = get_most_common_words_for_topics(lda, vectorizer, 10)\n","print_topic_words(topic_words)"]},{"cell_type":"code","execution_count":27,"id":"8c900e9d","metadata":{"id":"8c900e9d","executionInfo":{"status":"ok","timestamp":1670423341252,"user_tz":0,"elapsed":4,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["def test_new_example(lda, vect, example):\n","    vectorized = vect.transform([example])\n","    topic = lda.transform(vectorized)\n","    print(topic)\n","    return topic"]},{"cell_type":"code","execution_count":26,"id":"49a9a9a0","metadata":{"id":"49a9a9a0","executionInfo":{"status":"ok","timestamp":1670423332494,"user_tz":0,"elapsed":414,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# bbc news article\n","new_example = \"\"\"Gareth Southgate says England's situation is \"more complicated than any other country\" after announcing a 33-man provisional squad for Euro 2020.\n","The England manager must name a 26-man squad by 1 June.\n","Manchester United, Manchester City and Chelsea play European finals this week.\n","\"There are 12 players still to play so we're always going to need additional players and added to that we have some injuries at different stages, that we have very little info about,\" he said.\n","\"We felt more time will help us make better decisions. Our preference was to name the 26, but we have not got an ideal hand of cards - a lot of unknowns.\n","\"Info and evidence are very important and we will have a lot more in the next seven days.\"\n","White and Godfrey among uncapped quartet in 33-man provisional England squad\"\"\""]},{"cell_type":"code","execution_count":29,"id":"56e6afd2","metadata":{"id":"56e6afd2","executionInfo":{"status":"ok","timestamp":1670423362486,"user_tz":0,"elapsed":360,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}}},"outputs":[],"source":["# one of my blog posts\n","new_example = \"\"\"Last week I was honored and privileged to present at the ISKO UK meetup on the topic of ‘Searching, fast and slow’. This talk was a slightly updated version of the one I gave at Search Solutions 2020, in which I presented the case for a transformation of the systematic searching paradigm from the attributes on the left (which perpetuate ‘slow thinking’) to the attributes on the right (which facilitate ‘fast thinking’):\n","Procedural → Declarative\n","Static → Interactive\n","Monolithic → Executable\n","Strings → Objects\n","In that respect this talk aligns with the argument I presented at Search Solutions, but what is novel this time was the discussion afterwards: in particular, the suggestion that we could take the analogy further by exploring other software engineering concepts and practices that could inject further rigour, transparency and reproducibility into the systematic search/review process. What follows isn’t meant to be an exhaustive list, but rather an initial set, along with my thoughts on how they might potentially be applied (or have existing parallels) in the world of structured searching:\n","Concept\tDefinition\tSystematic searching equivalent\n","Design patterns\tGeneral, reusable solutions to commonly occurring problems\tFacet analysis schemas? E.g. PICO, SPICE, SPIDER, CIMO etc.\n","Continuous integration\tThe practice where members of a team integrate their work frequently into a shared master copy\tLiving systematic reviews? The review is updated frequently, and usually published as online-only \n","Git\tTooling for version control: tracking changes in files, coordinating work among team members\tCurrently no direct equivalent, but search strategies would benefit from version control and auditability\n","Github\tInternet hosting for development and version control\tPRESS forum? Currently no direct equivalent, but search strategy development would benefit from open access, cloud-based, reproducible solutions\n","Containers\tA way to package up code and its dependencies so the application runs reliably from one environment to another\tCurrently no direct equivalent, but search strategies would benefit from portability across databases\n","StackOverflow\tA community question and answer site to share solutions and best practices\tExpert searching mailing list? ISSG search filters resource? Currently no direct equivalent, but search strategy development would benefit from reusable solutions and knowledge sharing\n","Feel free to add to this list, they are just my initial thoughts. I should also give credit to the ISKO community for being the catalyst for this: I think it’s an intriguing thread, and one to which I will give further thought. In the meantime, my slides are attached below.\"\"\""]},{"cell_type":"code","execution_count":30,"id":"9a3e8494","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a3e8494","executionInfo":{"status":"ok","timestamp":1670423368742,"user_tz":0,"elapsed":631,"user":{"displayName":"Tony Russell-Rose","userId":"17399005977012450970"}},"outputId":"fc337dc3-bdc5-46ed-d2bd-ed18728bb15b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.68228456 0.31348018 0.00142233 0.00141268 0.00140025]]\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.68228456, 0.31348018, 0.00142233, 0.00141268, 0.00140025]])"]},"metadata":{},"execution_count":30}],"source":["test_new_example(lda, vectorizer, new_example)"]},{"cell_type":"markdown","id":"ee8bf7aa","metadata":{"id":"ee8bf7aa"},"source":["### Try different values of N"]},{"cell_type":"code","execution_count":null,"id":"f8e011d1","metadata":{"id":"f8e011d1"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}